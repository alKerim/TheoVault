**Lexical tokenization** is conversion of a text into (semantically or syntactically) meaningful _lexical tokens_ belonging to categories defined by a "lexer" program. In case of a natural language, those categories include nouns, verbs, adjectives, punctuations etc. In case of a programming language, the categories include identifiers, operators, grouping symbols and [data types](https://en.wikipedia.org/wiki/Data_type "Data type"). Lexical tokenization is related to the type of tokenization used in [[LLM Large Language Model|Large language models]] (LLMs), but with two differences. First, lexical tokenization is usually based on a [[Lexikalische Grammatik|lexical grammar]], whereas LLM tokenizers are usually [probability](https://en.wikipedia.org/wiki/Probability "Probability")-based. Second, LLM tokenizers perform a second step that converts the tokens into numerical values.

# Example
![[Screenshot 2024-04-26 at 17.42.53.png]]

[Source](https://en.wikipedia.org/wiki/Lexical_analysis#Token)
